fix_seed: 0
checkpoints_every: 64
tensorboard_log: True
tqdm_progress_bar: True
result_path: ./results/test/

model:
  compile_model: False
  in_channels: 3
  out_channels: 32
  num_layers: 4
  num_classes: 10
  num_blocks: 2
  growth_rate: 1.5
  conv_dropout: 0.2
  fc_dropout: 0

train_settings:
  data_path: /path/to/training_data/
  num_epochs: 128
  shuffle: True
  mixed_precision: bf16 # no, fp16, bf16, fp8
  batch_size: 128
  num_workers: 4
  grad_accumulation: 4
  grad_clip_norm: 1

valid_settings:
  data_path: /path/to/validation_data/
  batch_size: 8
  num_workers: 0

optimizer:
  name: adam
#  name: sgd
  lr: 1e-2
  weight_decouple: True
  weight_decay: 1e-2
  eps: 1e-16
  beta_1: 0.9
  beta_2: 0.999
  use_8bit_adam: False
  grad_clip_norm: 1
  decay: # presumably a cosine decay learning rate scheduler with warmup
    warmup: 512
    min_lr: 0
    gamma: 0.2
    num_restarts: 1
  nesterov: true
  momentum: 0.90

scheduler:
#    name: multistep_lr
#    name: cosine_annealing
#    name: cosine_annealing_warm_restarts
    name: cosine_annealing_sequential

    T_max: 128
    T_0: 16
    T_mult: 2
    eta_min: 0.0001
    eta_min_first: 0.0005
    eta_min_second: 0.00001
